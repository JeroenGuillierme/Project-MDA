{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32babd5f-a376-4bdc-ba15-7a9b548f9593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b639b6-13b7-4552-8fe7-91c89075738b",
   "metadata": {},
   "source": [
    "**Importing Datasets**\n",
    "\n",
    "Original 'dirty' datasets retrieved from AWS s3 bucket and aed dataset with geocoded addresses retrieved from public GitHub Repository (takes 1336s to run this data retrieval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71d7f692-4517-459a-ad76-870694e59ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 's3://mdaprojectdata2/'\n",
    "url2 = 'https://raw.githubusercontent.com/JeroenGuillierme/Project-MDA/main/Data/'\n",
    "\n",
    "ambulance = pd.read_parquet(f'{url}ambulance_locations.parquet.gzip')\n",
    "mug = pd.read_parquet(f'{url}mug_locations.parquet.gzip')\n",
    "pit = pd.read_parquet(f'{url}pit_locations.parquet.gzip')\n",
    "interventions1 = pd.read_parquet(f'{url}interventions1.parquet.gzip')\n",
    "interventions2 = pd.read_parquet(f'{url}interventions2.parquet.gzip')\n",
    "interventions3 = pd.read_parquet(f'{url}interventions3.parquet.gzip')\n",
    "interventions4 = pd.read_parquet(f'{url}interventions_bxl.parquet.gzip')\n",
    "interventions5 = pd.read_parquet(f'{url}interventions_bxl2.parquet.gzip')\n",
    "cad = pd.read_parquet(f'{url}cad9.parquet.gzip')\n",
    "aed = pd.read_parquet(f'{url}aed_locations.parquet.gzip')\n",
    "\n",
    "aed_total = pd.read_csv(f'{url2}aed_total_coordinates.csv')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.mode.copy_on_write = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8be4de-d2d4-47fb-89a1-15255306feae",
   "metadata": {},
   "source": [
    "**Functions**\n",
    "\n",
    "Here some functions are defined, which will be used further in this Notebook for preprocessing/cleaning the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "558806b2-9cfa-4fef-b136-5468a6166be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to correct latitude values\n",
    "def correct_latitude(lat):\n",
    "    '''\n",
    "    Corrects and standardizes latitude values by ensuring they are numeric and properly formatted.\n",
    "    :param lat: str, float or int\n",
    "    The latitude value to be corrected.\n",
    "    :return: float\n",
    "    The corrected latitude value, or NaN if the input is NaN.\n",
    "    '''\n",
    "    if pd.isna(lat):\n",
    "        return lat  # Return NaN as it is\n",
    "    if isinstance(lat, (str, float, int)):\n",
    "        lat_str = str(lat)\n",
    "        # Remove any existing non-numeric characters (except -)\n",
    "        lat_str = re.sub(r'[^0-9-]', '', lat_str)\n",
    "        # Move the decimal point to ensure two digits before the decimal point\n",
    "        if len(lat_str) > 2:\n",
    "            lat_str = lat_str[:2] + '.' + lat_str[2:]\n",
    "        return float(lat_str)\n",
    "    return lat\n",
    "\n",
    "\n",
    "# Function to correct longitude values\n",
    "def correct_longitude(lon):\n",
    "    '''\n",
    "    Corrects and standardizes longitude values by ensuring they are numeric and properly formatted.\n",
    "    :param lon: str, float or int\n",
    "    The longitude value to be corrected.\n",
    "    :return:float\n",
    "    The corrected longitude value, or NaN if the input is NaN.\n",
    "    '''\n",
    "    if pd.isna(lon):\n",
    "        return lon  # Return NaN as it is\n",
    "    if isinstance(lon, (str, float, int)):\n",
    "        lon_str = str(lon)\n",
    "        # Remove any existing non-numeric characters (except -)\n",
    "        lon_str = re.sub(r'[^0-9-]', '', lon_str)\n",
    "        # Move the decimal point to ensure one digit before the decimal point\n",
    "        if len(lon_str) > 1:\n",
    "            lon_str = lon_str[:1] + '.' + lon_str[1:]\n",
    "        return float(lon_str)\n",
    "    return lon\n",
    "\n",
    "\n",
    "# Define a function to extract the numeric part using regex\n",
    "def extract_numeric(text):\n",
    "    '''\n",
    "    Extracts the first numeric part from a given text using regular expressions.\n",
    "    :param text: str\n",
    "    The text from which to extract the numeric part.\n",
    "    :return: int or float\n",
    "    The extracted numeric value, or NaN if no numeric part is found.\n",
    "    '''\n",
    "    match = re.search(r'\\d+', text)\n",
    "    return int(match.group()) if match else np.nan\n",
    "\n",
    "\n",
    "# Function to filter rows based on coordinates falling within Belgium\n",
    "def is_within_belgium(lat, lon):\n",
    "    '''\n",
    "    Checks if given latitude and longitude coordinates fall within Belgium's geographical boundaries.\n",
    "    :param lat: float\n",
    "    The latitude value to be checked.\n",
    "    :param lon: float\n",
    "    The longitude value to be checked.\n",
    "    :return: bool\n",
    "    True if the coordinates are within Belgium's boundaries, False otherwise.\n",
    "    '''\n",
    "    # Define the geographical boundaries of Belgium\n",
    "    belgium_boundaries = {\n",
    "        'min_latitude': 48,\n",
    "        'max_latitude': 55,\n",
    "        'min_longitude': 2,\n",
    "        'max_longitude': 8\n",
    "    }\n",
    "\n",
    "    return (belgium_boundaries['min_latitude'] <= lat <= belgium_boundaries['max_latitude']) and \\\n",
    "        (belgium_boundaries['min_longitude'] <= lon <= belgium_boundaries['max_longitude'])\n",
    "\n",
    "# Function to convert time formats like %d%b%y:%H:%M:%S to date time\n",
    "def convert_format1(time1):\n",
    "    return pd.to_datetime(time1, format='%d%b%y:%H:%M:%S')\n",
    "\n",
    "# Function to convert time formats like %Y-%m-%d %H:%M:%S.%f to date time\n",
    "def convert_format2(time2):\n",
    "    return pd.to_datetime(time2, format='%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "# Function to convert time formats like %Y-%m-%d %H:%M:%S.%f %z to date time\n",
    "def convert_format3(time3):\n",
    "    return pd.to_datetime(time3, format='%Y-%m-%d %H:%M:%S.%f %z')\n",
    "\n",
    "\n",
    "# Function to convert Timedelta to minutes\n",
    "def timedelta_to_minutes(td):\n",
    "    '''\n",
    "    Converts a pandas Timedelta object to minutes.\n",
    "    :param td: pd.TimeDelta\n",
    "    The Timedelta object to be converted.\n",
    "    :return: float\n",
    "    The total duration in minutes.\n",
    "    '''\n",
    "    return td.total_seconds() / 60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac87798-06aa-44dc-9053-626d4d9dc283",
   "metadata": {},
   "source": [
    "**Preprocessing Interventions Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56d2be52-458d-4a51-8d02-f1a91df3803f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['mission_id', 'service_name', 'postalcode_permanence',\n",
      "       'cityname_permanence', 'streetname_permanence',\n",
      "       'housenumber_permanence', 'latitude_permanence', 'longitude_permanence',\n",
      "       'permanence_short_name', 'permanence_long_name', 'vector_type',\n",
      "       'eventtype_firstcall', 'eventLevel_firstcall', 'eventtype_trip',\n",
      "       'eventlevel_trip', 'postalcode_intervention', 'cityname_intervention',\n",
      "       'latitude_intervention', 'longitude_intervention', 't0', 't1',\n",
      "       't1confirmed', 't2', 't3', 't4', 't5', 't6', 't7', 't9',\n",
      "       'intervention_time_t1reported', 'waiting_time', 'intervention_duration',\n",
      "       'departure_time_t1reported', 'unavailable_time',\n",
      "       'name_destination_hospital', 'postalcode_destination_hospital',\n",
      "       'cityname_destination_hospital', 'streetname_destination_hospital',\n",
      "       'housenumber_destination_hospital', 'calculated_traveltime_departure_',\n",
      "       'calculated_distance_departure_to', 'calculated_traveltime_destinatio',\n",
      "       'calculated_distance_destination_', 'number_of_transported_persons',\n",
      "       'abandon_reason'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(interventions4.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9499e3e7-b6a7-4379-9bad-307f49caf01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting time formats to datetime \n",
    "\n",
    "interventions1[\"T0\"].apply(convert_format1)\n",
    "interventions2[\"T0\"].apply(convert_format1)\n",
    "interventions3[\"T0\"].apply(convert_format1)\n",
    "cad[\"T0\"].apply(convert_format2)\n",
    "interventions4[\"t0\"].apply(convert_format3)\n",
    "interventions5[\"t0\"].apply(convert_format1)\n",
    "\n",
    "interventions1[\"T3\"].apply(convert_format2)\n",
    "interventions2[\"T3\"].apply(convert_format2)\n",
    "interventions3[\"T3\"].apply(convert_format2)\n",
    "cad[\"T3\"].apply(convert_format2)\n",
    "interventions4[\"t3\"].apply(convert_format3)\n",
    "interventions5[\"t3\"].apply(convert_format1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38738502-015c-4989-995a-416fa9a63993",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_14748\\527659069.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  CAD9_expanded['Eventlevel'] = CAD9_expanded['EventLevel Trip']\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_14748\\527659069.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  CAD9_expanded['Latitude'] = CAD9_expanded['Latitude intervention'].apply(correct_latitude)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_14748\\527659069.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  CAD9_expanded['Longitude'] = CAD9_expanded['Longitude intervention'].apply(correct_longitude)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_14748\\527659069.py:45: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  interventions4[\"t0\"] = pd.to_datetime(interventions4[\"t0\"],\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_14748\\527659069.py:53: PerformanceWarning: Adding/subtracting object-dtype array to DatetimeArray not vectorized.\n",
      "  interventions4['T3-T0'] = interventions4['t3'] - interventions4['t0']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Latitude', 'Longitude', 'Intervention', 'CAD9', 'Eventlevel', 'T3-T0_min', 'AED', 'Ambulance', 'Mug', 'Occasional_Permanence']\n",
      "42       18.884917\n",
      "87        4.238700\n",
      "88       12.612550\n",
      "207      13.427900\n",
      "208      36.619500\n",
      "           ...    \n",
      "36258    10.183333\n",
      "36414     9.683333\n",
      "36415     7.733333\n",
      "38298          NaN\n",
      "38299          NaN\n",
      "Name: T3-T0_min, Length: 13521, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Append interventions dataset (1-3)\n",
    "interventions1[\"T0\"] = pd.to_datetime(interventions1[\"T0\"],\n",
    "                                      format='%d%b%y:%H:%M:%S')  # Convert the first time format to datetime\n",
    "interventions1[\"T3\"] = pd.to_datetime(interventions1[\"T3\"],\n",
    "                                      format='%Y-%m-%d %H:%M:%S.%f')  # Convert the column back to datetime with the new format\n",
    "interventions2[\"T0\"] = pd.to_datetime(interventions2[\"T0\"],\n",
    "                                      format='%d%b%y:%H:%M:%S')  # Convert the first time format to datetime\n",
    "interventions2[\"T3\"] = pd.to_datetime(interventions2[\"T3\"],\n",
    "                                      format='%Y-%m-%d %H:%M:%S.%f')  # Convert the column back to datetime with the new format\n",
    "interventions3[\"T0\"] = pd.to_datetime(interventions3[\"T0\"],\n",
    "                                      format='%d%b%y:%H:%M:%S')  # Convert the first time format to datetime\n",
    "interventions3[\"T3\"] = pd.to_datetime(interventions3[\"T3\"],\n",
    "                                      format='%Y-%m-%d %H:%M:%S.%f')  # Convert the column back to datetime with the new format\n",
    "interventions_total = pd.concat([interventions1, interventions2, interventions3], axis=0)\n",
    "\n",
    "interventions_total['CAD9'] = 0\n",
    "interventions_total[\"T3-T0\"] = interventions_total[\"T3\"] - interventions_total[\"T0\"]\n",
    "interventions_total = interventions_total[interventions_total[\"EventType Firstcall\"] == \"P003 - Cardiac arrest\"]\n",
    "interventions_total['Eventlevel'] = interventions_total[\"EventLevel Firstcall\"]\n",
    "interventions_total['Latitude'] = interventions_total['Latitude intervention'].apply(correct_latitude)\n",
    "interventions_total['Longitude'] = interventions_total['Longitude intervention'].apply(correct_longitude)\n",
    "interventions_total['Intervention'] = 1\n",
    "interventions_total = interventions_total[\n",
    "    ['Latitude', 'Longitude', \"Intervention\", \"CAD9\", \"Eventlevel\", \"T3-T0\"]]\n",
    "\n",
    "# Append Interventions CAD9 dataset\n",
    "CAD9_expanded = cad\n",
    "CAD9_expanded['CAD9'] = 1  # add extra column CAD9\n",
    "CAD9_expanded['Intervention'] = 1  # add extra column Intervention\n",
    "CAD9_expanded[\"T0\"] = pd.to_datetime(CAD9_expanded[\"T0\"],\n",
    "                                     format='%Y-%m-%d %H:%M:%S.%f')  # Convert the column back to datetime with the new format\n",
    "CAD9_expanded[\"T3\"] = pd.to_datetime(CAD9_expanded[\"T3\"],\n",
    "                                     format='%Y-%m-%d %H:%M:%S.%f')  # Convert the column back to datetime with the new format\n",
    "CAD9_expanded[\"T3-T0\"] = CAD9_expanded[\"T3\"] - CAD9_expanded[\"T0\"]\n",
    "CAD9_expanded.loc[\n",
    "    CAD9_expanded['T3-T0'] < pd.Timedelta(0), 'T3-T0'] = pd.NaT  # Transform negative time difference to NaT\n",
    "CAD9_expanded = CAD9_expanded[CAD9_expanded[\"EventType Trip\"] == \"P003 - HARTSTILSTAND - DOOD - OVERLEDEN\"]\n",
    "CAD9_expanded['Eventlevel'] = CAD9_expanded['EventLevel Trip']\n",
    "CAD9_expanded['Latitude'] = CAD9_expanded['Latitude intervention'].apply(correct_latitude)\n",
    "CAD9_expanded['Longitude'] = CAD9_expanded['Longitude intervention'].apply(correct_longitude)\n",
    "CAD9_expanded = CAD9_expanded[['Latitude', 'Longitude', 'Intervention', 'CAD9', 'Eventlevel', 'T3-T0']]\n",
    "\n",
    "# Append datasets interventions Brussels\n",
    "# Interventions 4\n",
    "interventions4[\"t0\"] = pd.to_datetime(interventions4[\"t0\"],\n",
    "                                      format='%Y-%m-%d %H:%M:%S.%f %z')  # Convert the first time format to datetime\n",
    "interventions4[\"t3\"] = pd.to_datetime(interventions4[\"t3\"],\n",
    "                                      format='%Y-%m-%d %H:%M:%S.%f %z')  # Convert the column back to datetime with the new format\n",
    "interventions4['CAD9'] = 0\n",
    "interventions4['Intervention'] = 1\n",
    "interventions4 = interventions4[interventions4['eventtype_firstcall'] == 'P003 - Cardiac arrest']\n",
    "interventions4['Eventlevel'] = interventions4['eventLevel_firstcall']\n",
    "interventions4['T3-T0'] = interventions4['t3'] - interventions4['t0']\n",
    "interventions4['Latitude'] = interventions4['latitude_intervention'].apply(correct_latitude)\n",
    "interventions4['Longitude'] = interventions4['longitude_intervention'].apply(correct_longitude)\n",
    "interventions4 = interventions4[['Latitude', 'Longitude', 'Intervention', 'CAD9', 'Eventlevel', 'T3-T0']]\n",
    "\n",
    "# Interventions 5\n",
    "interventions5[\"T0\"] = pd.to_datetime(interventions5[\"T0\"],\n",
    "                                      format='%d%b%y:%H:%M:%S')  # Convert the first time format to datetime\n",
    "interventions5[\"T3\"] = pd.to_datetime(interventions5[\"T3\"],\n",
    "                                      format='%d%b%y:%H:%M:%S')  # Convert the column back to datetime with the new format\n",
    "interventions5['CAD9'] = 0\n",
    "interventions5['Intervention'] = 1\n",
    "interventions5 = interventions5[\n",
    "    (interventions5['EventType and EventLevel'] == 'P003  N01 - HARTSTILSTAND - DOOD - OVERLEDEN') | (\n",
    "            interventions5['EventType and EventLevel'] == 'P003  N05 - HARTSTILSTAND - DOOD - OVERLEDEN')]\n",
    "interventions5[\"Eventlevel\"] = interventions5[\"EventType and EventLevel\"].str.split(\" \").str[1].str.replace(\"0\", \"\")\n",
    "interventions5['T3-T0'] = interventions5['T3'] - interventions5['T0']\n",
    "interventions5['Latitude'] = interventions5['Latitude intervention'].apply(correct_latitude)\n",
    "interventions5['Longitude'] = interventions5['Longitude intervention'].apply(correct_longitude)\n",
    "interventions5 = interventions5[['Latitude', 'Longitude', 'Intervention', 'CAD9', 'Eventlevel', 'T3-T0']]\n",
    "\n",
    "# Putting everything together\n",
    "interventions_TOTAL = pd.concat([interventions_total, CAD9_expanded, interventions4, interventions5], axis=0)\n",
    "interventions_TOTAL['AED'] = 0  # Adding extra AED column\n",
    "interventions_TOTAL['Eventlevel'] = interventions_TOTAL['Eventlevel'].apply(extract_numeric)\n",
    "interventions_TOTAL['Ambulance'] = 0\n",
    "interventions_TOTAL['Mug'] = 0\n",
    "interventions_TOTAL['Occasional_Permanence'] = np.nan\n",
    "\n",
    "# Filter out rows where T3-T0 is greater than one day => Changed by IsolationForest algorithm\n",
    "# one_day = pd.Timedelta(days=1)\n",
    "# one_hour = pd.Timedelta(minutes=55)\n",
    "# interventions_TOTAL = interventions_TOTAL[interventions_TOTAL['T3-T0'] < one_hour] # We lose approx. 4000 interventions with a response tie longer then one day\n",
    "\n",
    "interventions_TOTAL['T3-T0_min'] = interventions_TOTAL['T3-T0'].apply(timedelta_to_minutes)\n",
    "interventions_TOTAL = interventions_TOTAL[\n",
    "    ['Latitude', 'Longitude', 'Intervention', 'CAD9', 'Eventlevel', 'T3-T0_min', 'AED', 'Ambulance', 'Mug',\n",
    "     'Occasional_Permanence']]\n",
    "print(list(interventions_TOTAL.columns))\n",
    "print(interventions_TOTAL['T3-T0_min'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b7fa0e-12f9-4389-a666-73e037427dbe",
   "metadata": {},
   "source": [
    "**Preprocessing AED Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ef6fa2-a481-48a6-8b69-e37a9fc54fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "aed_total = pd.concat([aed_1, aed_2, aed_3, aed_4, aed_5], axis=0)\n",
    "yes_values = ['Y', 'y', 'Oui-Ja', 'Ja', 'Oui', 'J', np.nan]\n",
    "aed_total = aed_total[\n",
    "    aed_total['public'].isin(yes_values)]  # discard non-public aed's and also when no data is available\n",
    "\n",
    "aed_total['Latitude'] = aed_total['latitude'].apply(correct_latitude)\n",
    "aed_total['Longitude'] = aed_total['longitude'].apply(correct_longitude)\n",
    "aed_total['CAD9'] = 0\n",
    "aed_total['Intervention'] = 0\n",
    "aed_total['AED'] = 1\n",
    "aed_total['Eventlevel'] = np.nan\n",
    "aed_total['T3-T0_min'] = pd.NaT\n",
    "aed_total['Ambulance'] = 0\n",
    "aed_total['Mug'] = 0\n",
    "aed_total['Occasional_Permanence'] = np.nan\n",
    "aed_total = aed_total[\n",
    "    ['Latitude', 'Longitude', 'Intervention', 'CAD9', 'Eventlevel', 'T3-T0_min', 'AED', 'Ambulance', 'Mug',\n",
    "     'Occasional_Permanence']]\n",
    "\n",
    "# Apply the filter function to the aed_total\n",
    "aed_total2 = aed_total[aed_total.apply(lambda row: is_within_belgium(row['Latitude'], row['Longitude']), axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b69a4a9-2eab-411b-94a8-ff55b9ac31b9",
   "metadata": {},
   "source": [
    "**Preprocessing Ambulance Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc39f9a6-eb68-4819-9ed8-12bddb22ac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "ambulance['Latitude'] = ambulance['latitude'].apply(correct_latitude)\n",
    "ambulance['Longitude'] = ambulance['longitude'].apply(correct_longitude)\n",
    "ambulance['Occasional_Permanence'] = ambulance['occasional_permanence'].replace({'N': 0, 'Y': 1})\n",
    "ambulance['CAD9'] = 0\n",
    "ambulance['Intervention'] = 0\n",
    "ambulance['AED'] = 0\n",
    "ambulance['Eventlevel'] = np.nan\n",
    "ambulance['T3-T0_min'] = pd.NaT\n",
    "ambulance['Ambulance'] = 1\n",
    "ambulance['Mug'] = 0\n",
    "ambulance = ambulance[\n",
    "    ['Latitude', 'Longitude', 'Intervention', 'CAD9', 'Eventlevel', 'T3-T0_min', 'AED', 'Ambulance', 'Mug',\n",
    "     'Occasional_Permanence']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bc0fd4-9184-4430-98d5-1c03762c9550",
   "metadata": {},
   "source": [
    "**Preprocessing Mug Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1414657-85e0-4b84-a411-be8d93ea4816",
   "metadata": {},
   "outputs": [],
   "source": [
    "mug1['Latitude'] = mug1['latitude'].apply(correct_latitude)\n",
    "mug1['Longitude'] = mug1['longitude'].apply(correct_longitude)\n",
    "mug1['Occasional_Permanence'] = 1  # we assume the mug comes from the hospital and is permanently available.\n",
    "mug1['CAD9'] = 0\n",
    "mug1['Intervention'] = 0\n",
    "mug1['AED'] = 0\n",
    "mug1['Eventlevel'] = np.nan\n",
    "mug1['T3-T0_min'] = pd.NaT\n",
    "mug1['Ambulance'] = 1\n",
    "mug1['Mug'] = 1\n",
    "mug1 = mug1[['Latitude', 'Longitude', 'Intervention', 'CAD9', 'Eventlevel', 'T3-T0_min', 'AED', 'Ambulance', 'Mug',\n",
    "             'Occasional_Permanence']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83d0753-1e51-4cbd-aa94-5d4abdd7892d",
   "metadata": {},
   "source": [
    "**All together for AED optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b497aac7-1c93-4893-9cdf-1c444086eaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(aed_total2.columns))\n",
    "print(list(interventions_TOTAL.columns))\n",
    "print(list(ambulance.columns))\n",
    "print(list(mug1.columns))\n",
    "aed_df = pd.concat([interventions_TOTAL, aed_total2, ambulance, mug1], axis=0)\n",
    "\n",
    "print(aed_df.head())\n",
    "print(len(aed_df))\n",
    "print('Unique number of values Latitude values: ', len(list(aed_df['Latitude'].unique())))\n",
    "print('Unique number of values Longitude values: ', len(list(aed_df['Longitude'].unique())))\n",
    "print('Unique values eventlevels', aed_df['Eventlevel'].unique())\n",
    "print('Unique values CAD9: ', aed_df['CAD9'].unique())\n",
    "cross_tab = pd.crosstab(index=pd.Categorical(aed_df[\"Eventlevel\"]), columns='count')\n",
    "print('Cross table of Event Levels: \\n', cross_tab)\n",
    "\n",
    "max_responseTime = aed_df['T3-T0_min'].max()\n",
    "print(f\"Maximum Response Time for aed_df: {max_responseTime}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208baac9-0299-4934-854c-4d46dd6fca40",
   "metadata": {},
   "source": [
    "**Outlier Detection for Response Time T3-T0**\n",
    "\n",
    "IsolationForest (for outliers/anomalies of the response time T3-T0, because some are longer than 1 day)\n",
    "=> probably incorrectly filled in, in the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58348ee2-6196-476a-9be2-72dc329b3063",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aed_df['T3-T0_min'].isna().sum())  # 17445 NaN values\n",
    "\n",
    "# Split the DataFrame into two: one with the NaN and one without in the 'T3-T0_min' column\n",
    "aed_df_with_nan = aed_df[aed_df['T3-T0_min'].isna()]  # DataFrame with NaN values\n",
    "aed_df_without_nan = aed_df[~aed_df['T3-T0_min'].isna()]  # DataFrame without NaN values\n",
    "print('Without NaN: ', len(aed_df_without_nan))\n",
    "print('With NaN: ', len(aed_df_with_nan))\n",
    "\n",
    "Time = aed_df_without_nan['T3-T0_min']\n",
    "\n",
    "# IsolationForest algorithm\n",
    "IsoFo = IsolationForest(n_estimators=100, contamination='auto',\n",
    "                        random_state=45)  # Random state added for reproducibility\n",
    "y_labels = IsoFo.fit_predict(np.array(Time).reshape(-1, 1))\n",
    "\n",
    "# Only including the inliers\n",
    "aed_df_filtered = aed_df_without_nan[y_labels == 1]  # DataFrame with inliers\n",
    "discarded_rows = aed_df_without_nan[y_labels == -1]  # DataFrame with outliers\n",
    "\n",
    "min_timedelta = discarded_rows['T3-T0_min'].min()  # Time deltas larger than 44.27 minutes are discarded\n",
    "max_timedelta = discarded_rows['T3-T0_min'].max()\n",
    "min_timedelta2 = aed_df_filtered['T3-T0_min'].min()\n",
    "max_timedelta2 = aed_df_filtered['T3-T0_min'].max()\n",
    "print(f\"min_timedelta of outliers: {min_timedelta}\")  # Minimum outlier value = 44.27 minutes\n",
    "print(f\"max_timedelta of outliers: {max_timedelta}\")  # Maximum outlier value = 80267.83 minutes\n",
    "print(f\"min_timedelta of inliers: {min_timedelta2}\")  # Minimum outlier value = 0.62 minutes\n",
    "print(f\"max_timedelta of inliers: {max_timedelta2}\")  # Maximum outlier value = 44.17 minutes\n",
    "print(f\"Number of discarded rows: {len(discarded_rows)}\")  # 1040\n",
    "print(f\"Number of filtered rows: {len(aed_df_filtered)}\")  # 9306\n",
    "\n",
    "print(\"\\nFiltered DataFrame (Inliers):\")\n",
    "# print(aed_df_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e189ee-92a8-48b2-80d1-1d6fc2a58907",
   "metadata": {},
   "source": [
    "**Total dataset without outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6679c09d-baea-4b97-b7b0-e0fc463f6c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add rows with NaN values again:\n",
    "print(list(aed_df_filtered.columns))\n",
    "print(list(aed_df_with_nan.columns))\n",
    "aed_ready = pd.concat([aed_df_filtered, aed_df_with_nan], axis=0)\n",
    "\n",
    "x = range(0, 9306)\n",
    "plt.scatter(x, aed_df_filtered['T3-T0_min'])\n",
    "plt.show()\n",
    "\n",
    "# Last check if no weird values are included in the dataset\n",
    "\n",
    "# Get the minimum and maximum values of the 'latitude' column\n",
    "min_latitude = aed_ready['Latitude'].min()\n",
    "max_latitude = aed_ready['Latitude'].max()\n",
    "# Get the minimum and maximum values of the 'longitude' column\n",
    "min_longitude = aed_ready['Longitude'].min()\n",
    "max_longitude = aed_ready['Longitude'].max()\n",
    "# Get the minimum and maximum values of the response time column\n",
    "min_timedelta = aed_ready['T3-T0_min'].min()\n",
    "max_timedelta = aed_ready['T3-T0_min'].max()\n",
    "\n",
    "print('Length of dataset: ', len(aed_ready))\n",
    "print('Number of missing values per column: \\n', print(aed_ready[['Latitude', 'Longitude', 'Intervention', 'CAD9',\n",
    "                                                                  'Eventlevel', 'T3-T0_min', 'AED', 'Ambulance', 'Mug',\n",
    "                                                                  'Occasional_Permanence']].isna().sum()))  # 1210 missing values for latitude from intervention dataset\n",
    "print(f\"Minimum latitude of dataset: {min_latitude}\")\n",
    "print(f\"Maximum latitude of dataset: {max_latitude}\")\n",
    "print(f\"Minimum longitude of dataset: {min_longitude}\")\n",
    "print(f\"Maximum longitude of dataset: {max_longitude}\")\n",
    "print(f\"min_timedelta of dataset: {min_timedelta}\")  # Minimum outlier value = 44.27 minutes\n",
    "print(f\"max_timedelta of dataset: {max_timedelta}\")  # Maximum outlier value = 80267.83 minutes\n",
    "\n",
    "# aed_ready.to_csv('C:/Users/Admin/Documents/GitHub/Project-AED-optimalization/DATA/aed_placement_df.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MDA24",
   "language": "python",
   "name": "mda24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
